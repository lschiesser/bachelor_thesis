This chapter presents the results of the reproduction of the classifiers 
according to the procedure described in \hyperref[chap:methods]{Chapter 2}. In 
addition, the performance of the reproduced classifiers and the classifiers 
from the original work \cite{RN127} are compared. 
\section{Results of the Reproduction}
Table \ref{tab:training-acc} and \ref{tab:training-b_acc} show the mean and 
standard deviation for the models' accuracy and balanced accuracy during 
nested cross-validation. The authors of the original paper provide 95\% 
confidence intervals for these two measures. Since there is no information on 
how they computed this interval, it is assumed that they used a two-sided 
Student's t-distribution, which is used to estimate the mean of normally 
distributed populations where the sample size is small. It is calculated using 
Equation \ref{eq:t-dist} where 
$t_{\alpha, v}$ is the t-value which can be taken from a table with $\alpha$ 
as the degrees of freedom n-1 and $v$ as the confidence level, $s$ is the 
sample variance, $\bar{x}$ is the sample mean, and $n$ is the number of samples 
\cite{RN197}.
\begin{equation}
 \bar{x} \pm \frac{t_{\alpha, v}s}{\sqrt{n}}
 \label{eq:t-dist}
\end{equation}
For the Random Forest, the 95\% confidence interval of the average accuracy is 
[0.76, 0.80], and of the average balanced accuracy is [0.73, 0.77]. 
For Logistic Regression, the confidence interval of the average accuracy is 
[0.72, 0.77], and of the average balanced accuracy is [0.68, 0,72]. Since the 
confidence intervals of the accuracy and balanced accuracy for both classifiers 
overlap or are near each other, it can be concluded that the slight imbalance 
of the data set had no significant effect on the classifiers and that the 
classifiers, therefore, are not biased against the outcomes of the target 
variable. All evaluation metrics for each fold and imputed data set are 
provided in Table \ref{tab:storage}. There are no metrics to report for the 
Decision Tree since it is only trained during the validation due to being 
primarily used to approximate the Random Forest's decision-making 
process.
\\
The best models were selected by searching for the model with the highest 
evaluation measures. The best Random Forest Classifier used 
maximally 
four features, 500 estimators, and Gini impurity. It achieved an accuracy of 
87.5\% and a 
sensitivity of 90\%. The best Logistic Regression Classifier used a C of 10, a 
maximal iteration of 200, the L2 penalty, and the 'liblinear' solver. It 
achieved an accuracy 
of 83.9\% and a sensitivity of 85\%. These settings were then used to retrain 
the classifiers and compute the validation metrics.
\\
% look at this again and see if there are things you could change
The Random Forest Classifier is the best performing classifier after 
retraining it with the best hyperparameters, with an accuracy of 76.8\% and a 
balanced accuracy of 73.7\%. The Logistic Regression achieves an accuracy of 
73.2\% and a balanced accuracy of 69.4\%. The Decision Tree only 
achieves an accuracy of 71.4\% and a balanced accuracy of 68.5\%. 
Accordingly, there is no significant difference between the accuracy and 
balanced accuracy for all three classifiers, meaning no classifier is biased.
\\
All single-threshold metrics are provided in Table \ref{tab:validation-metrics}.
The two threshold-free metrics, ROC and PRC curve, are shown in Figure 
\ref{fig:prc-roc}. Both plots show that the classifiers perform better than the 
reference line. The values for the area under the ROC and under the PRC curve 
suggest that Logistic Regression (AUC: 80.8\%, AUC (PRC): 85.5\%) performs 
better than the Random Forest (AUC: 80.5\%, AUC (PRC): 82.9\%) although all 
other metrics indicate better performance of the latter. It 
should be noted that the difference between the AUC scores for both curves is 
minimal and is not meaningful enough to declare the Logistic Regression as the 
better classifier. The more apparent differences in the AUC (PRC) for both 
classifiers, on the other hand, could indicate that the Logistic Regression is 
less affected by the imbalance in the data set than the Random Forest is.
The Decision Tree Classifier exhibits a lower discriminative performance,
as indicated by the AUC and AUC (PRC), with 68.5\% and 69.8\%, 
indicating a relatively weak classifier. Despite these low metrics, the 
Decision Tree can still help clinicians make quick decisions and give an 
approximation of the Random Forest.
\par
Figure \ref{fig:feature-importance} shows the feature importance plots for the 
three classifiers. The feature importance for the Decision Tree and Random 
Forest was obtained using the \code{sklearn} implementation. The implementation 
calculates 
the feature importance by quantifying the mean decrease in impurity. 
It should be noted that impurity-based feature importance can suffer 
from favoring features with a high number of unique values and from making 
predictions based on statistics derived from the training set. Therefore, this 
method is not necessarily informative about whether a feature makes a 
good prediction or not \cite{RN178}.
For Logistic Regression, the feature importance is inferred by looking at 
each feature's coefficients in log space. A positive coefficient can be 
interpreted as contributing to a positive classification (1), and a negative 
coefficient can be interpreted as contributing to a negative classification (0) 
\cite{RN174}.
It is difficult to compare non tree-based feature importance with tree-based 
importance because they have different scales and calculations. Nonetheless, 
the 
coefficients' absolute values in log space can be analyzed alongside the 
tree-based methods' impurity-based importance features.
\\
Both tree-based methods show a qualitatively similar feature importance plot, 
with AST being the most important feature. The remaining features are 
differently distributed but mostly exhibit similar values. Consequently, the 
Decision Tree and its visualization (Figure \ref{fig:dt}), combined with the 
feature importance plots for both tree-based classifiers, can be used to gain 
some rough insights into the Random Forest classification procedure. 
\hyperref[sec:medical]{Section 4.1.2} will take a closer look at the feature 
importance of the three classifiers and compare them to findings from medical 
research. The Logistic Regression's feature importance plot shows that the 
Eosinophil count is the most important feature.

\section{Comparison with the Original Paper}
The 95\% confidence intervals for the accuracy and balanced accuracy of this 
replication overlap with the ones from the original paper. The average accuracy 
for the Random Forest in the original is [0.74, 0.80] (replication: [0.76, 
0.80]) 
and [0.70, 0.81] (replication: [0.72, 0.77]) for Logistic Regression. The 
average balanced accuracy in the original work is [0.70, 0.82] (replication: 
[0.73, 0.77]) for Random Forest and [0.65, 0.74] (replication: [0.68, 0.72]) 
for Logistic Regression. Since there is no information about individual values 
during the nested cross-validation and it is also not reported how the authors 
computed the 95\% confidence intervals, it can be difficult to compare them. 
Nevertheless, the overlap of the intervals suggests that the classifiers can be 
compared to a certain degree. Moreover, the replication's intervals are subsets 
of the original's intervals and are more narrow than the original ones 
suggesting that the replication's classifiers provide a more precise 
classification.
Unfortunately, there is no more information about other evaluation 
metrics.
\\
The validation metrics of the final classifiers show lower performance than the 
classifiers in the original paper. In this replication, the Random Forest has an 
accuracy of 0.77, and the Logistic Regression an accuracy of 0.73. In the 
original work, the Random Forest achieves an accuracy of 0.82, and the Logistic 
Regression an accuracy of 0.78. Though the replicated classifiers perform worse 
than the original ones, their accuracies exhibit a similar difference of 0.04. 
Further, all single-threshold metrics except for the AUC are higher for the 
Random Forest in the replication. The sensitivity is equal for both classifiers 
in the reproduction.  In contrast, in the original work, only accuracy and 
specificity are higher for the Random Forest, AUC and sensitivity are higher for 
the Logistic Regression. When setting aside the sensitivity, the classifiers 
show a similar pattern where accuracy and specificity are better for the Random 
Forest and AUC is better for the Logistic Regression. Therefore, the replicated 
classifiers show a similar but less accurate performance than classifiers in the 
original work. The ROC plot for both classifiers (Figure \ref{fig:roc}) 
exhibits a similar course as in the original paper since all classifiers 
operate above the reference line (chance). Still, the higher AUC of the 
original classifiers is also reflected in the original's plot because they 
reach further 
towards the upper left corner. Seeing that the precision-recall curve only 
plots the 
Random Forest's performance, we can only compare these results. The replication 
(Figure \ref{fig:prc}) and the original study differ considerably in this plot. 
The replicated classifier has a more abrupt course due to fewer 
computed thresholds which are selected by the function computing the curve. 
Nonetheless, both curves exhibit similar behavior as both 
curves drop rapidly in the interval recall = [0.0, 0.2], suggesting a reduced 
classification rate that improves afterward. Unfortunately, there was no 
information regarding the validation metrics of the Decision Tree classifier 
in the original paper. 
Nevertheless,  the reproduced classifier's validation metrics can be compared 
to the cross-validation's 95\% confidence intervals of the original's model 
training phase.
The reproduced Decision Tree achieves an accuracy of 0.71, which lies in the 
confidence interval [0.70, 0.78], and a balanced accuracy of 0.69, which lies 
in the confidence interval [0.64, 0.71].
