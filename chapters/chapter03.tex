\section{Results of Own Implementation}
Table \ref{tab:training-acc} and \ref{tab:training-b_acc} show the mean and 
standard deviation for models' the accuracy and balanced accuracy during 
nested cross-validation. The authors of the original paper provided a 95\% 
confidence interval for these two measures. Since there is no information on 
how they computed this interval, it is assumed that they used a two-sided 
Student's t-distribution, which is used to estimate the mean of normally 
distributed populations where the sample size is small. It is calculated using 
Equation \ref{eq:t-dist} where 
$t_{\alpha, v}$ is the t-value which can be taken from a table with $\alpha$ 
as the degrees of freedom $n-1$ and $v$ as the confidence level, $s$ is the 
sample variance, $\bar{x}$ is the sample mean, and $n$ is the number of samples 
\cite{RN197}.
For the Random Forest, the 95\% confidence interval of the average accuracy is 
[0.76, 0.80], andof the average balanced accuracy is [0.73, 0.77]. 
For Logistic Regression, the confidence interval of the average accuracy is 
[0.72, 0.77], and of the average balanced accuracy is [0.68, 0,72]. Since the 
confidence intervals of the accuracy and balanced accuracy for both classifiers 
overlap or are near each other, it can be assumed that the slight imbalance of 
the data set had no significant effect on the classifiers and that the 
classifiers, therefore, are not biased against the outcomes of the target 
variable. All evaluation metrics for each fold and imputed data set are 
provided in Table \ref{tab:storage}.
\begin{equation}
 \bar{x} \pm \frac{t_{\alpha, v}s}{\sqrt{n}}
 \label{eq:t-dist}
\end{equation}
The best models were selected by searching for the model with the highest 
evaluation measures. The best Random Forest Classifier used 
maximally 
four features, 500 estimators, and Gini impurity. It achieved an accuracy of 
87.5\% and a 
sensitivity of 90\%. The best Logistic Regression Classifier used a C of 10, a 
maximal iteration of 200, the 'l2' penalty, and the 'liblinear' solver. It 
achieved an accuracy 
of 83.9\% and a sensitivity of 85\%. These settings were then used to retrain 
the classifiers and compute the validation metrics.
\\
% look at this again and see if there are things you could change
The Random Forest Classifier is also the best performing classifier after 
retraining it with the best hyperparameter with an accuracy of 76.8\% and a 
balanced accuracy of 73.7\%. The Logistic Regression achieves an accuracy of 
73.2\% and a balanced accuracy of 69.4\%. Accordingly, there is 
no significant difference between the accuracy and balanced accuracy for 
all three classifiers, meaning no classifier is biased.
The metrics mentioned above are lower than the evaluation metrics since the 
data 
set used to train during the validation phase is different from the one used 
during the evaluation phase due to the way MICE imputes missing data.
\\
All single-threshold metrics are provided in Table \ref{tab:validation-metrics}.
The two threshold-free metrics, ROC and PRC curve, are shown in Figure 
\ref{fig:prc-roc}. Both plots show that the classifiers perform better than the 
reference line. The values for the area under the ROC and under the PRC curve 
suggest that Logistic Regression (AUC: 80.8\%, AUC (PRC): 85.5\%) performs 
better than the Random Forest (AUC: 80.5\%, AUC (PRC): 82.9\%) although all 
other metrics indicate better performance of the latter. It 
should be noted that the difference between the AUC scores for both curves is 
minimal and is not meaningful enough to declare the Logistic Regression as the 
better classifier. The more apparent differences in the AUC (PRC) for both 
classifiers, on the other hand, could indicate that the Logistic Regression is 
less affected by the imbalance in the data set than the Random Forest is.
\\
The Decision Tree Classifier exhibits a lower discriminative performance,
as indicated by the AUC and AUC (PRC) with 68.5\% and 69.8\% 
indicating a relatively weak classifier. Additionally, it only 
achieves an accuracy of 71.4\% and a balanced accuracy of 68.5\%. Despite 
these low metrics, the Decision Tree can still be used to 
help clinicians make quick decisions and interpret the result of the Random 
Forest.
\par
Figure \ref{fig:feature-importance} shows the feature importance plots for the 
three classifiers. The feature importance for the Decision Tree and Random 
Forest was obtained using the sklearn implementation. The implementation 
calculates 
the feature importance by quantifying the mean decrease in impurity. 
It should be noted that impurity-based feature importance can suffer 
from favoring features with a high number of unique values and from making 
predictions based on statistics derived from the training set. Therefore, this 
method is not necessarily informative about whether a feature makes a 
good prediction or not \cite{RN178}.
For Logistic Regression, the feature importance is inferred by looking at the 
each feature's coefficients in log space. A positive coefficient can be 
interpreted as contributing to a positive (1) classification, and a negative 
coefficient can be interpreted as contributing to a negative (0) classification.
It is difficult to compare non tree-based feature importance with tree-based 
importance because they have different scales and calculations. Nontheless, the 
absolute values of the coefficients in log space can be compared with the 
impurity-based importance features of the tree based-methods
\\
Both tree-based methods show a similar feature importance plot, with AST being 
the most important feature. The remaining features are differently distributed 
but mostly exhibit similar values. Consequently, the Decision Tree and its 
visualization (Figure \ref{fig:dt}), combined with the feature 
importance plots for both tree-based classifiers, can be used to give some 
rough insights into the Random Forest classification procedure. 
\hyperref[sec:medical]{Chapter 4.1} will take a closer look at the feature 
importance of the three classifiers and compare them to findings from medical 
research.

\section{Comparison with Original Paper}
The 95\% confidence intervals for the accuracy and balanced accuracy of this 
replication overlap with the ones from the original paper. The average accuracy 
for the Random Forest in the original is [0.74, 0.80] (replication: [0.76, 
0.80]) 
and [0.70, 0.81] (replication: [0.72, 0.77]) for Logistic Regression. The 
average balanced accuracy in the original work is [0.70, 0.82] (replication: 
[0.73, 0.77]) for Random Forest and [0.65, 0.74] (replication: [0.68, 0.72]) 
for Logistic Regression. Since there is no information about individual values 
during the nested cross validation and it is also not reported how the authors 
computed the 95\% confidence intervals, it can be difficult to compare them. 
Nevertheless, the overlap of the intervals suggests that the classifiers can be 
compared to a certain degree. Since the confidence interval of the replication 
is in the original's confidence interval, the performance of the 
classifiers during the cross-validation can be compared. Unfortunately, there 
is no more information about other evaluation metrics in the original paper.
\\
The validation metrics of the final classifiers show lower performance 
than the classifiers in the original paper. However, the differences between 
the Random Forest and Logistic Regression are similar to the ones in the 
original paper. In this replication, the single-threshold metrics are higher 
for the Random Forest. The threshold-free metrics are higher for the Logistic 
Regression, reflecting a behavior similar to that in the original paper. The 
ROC curve for both classifiers exhibits a similar course as in the original, 
except that the curves in the original rise higher in the beginning due to 
their higher discriminative power. Seeing that the precision-recall curve only 
plots the Random Forest's performance, we can only compare these results. 
The replication and the original differ considerably in this plot, but still 
exhibit a similar course at some points, e.g., the sharp drop between 
sensitivity [0.0, 0.2]. There is no data regarding the validation metrics of 
the Decision Tree
