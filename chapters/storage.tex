\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{llllrrrrrr}
Fold & Imputed Set & Model & Model Count &   Accuracy &  Balanced Accuracy &  
Sensitivity &  
Specificity & PPV & AUC \\ \hline
   0 &           1 &    lr &           0 & 0.750 &  0.709 &        0.939 &       
 0.478 & 0.721 & 0.816 \\
   0 &           2 &    lr &           1 & 0.714 &  0.679 &        0.879 &       
 0.478 & 0.707 & 0.837 \\
   0 &           3 &    lr &           2 & 0.696 &  0.663 &        0.848 &       
 0.478 & 0.700 & 0.806 \\
   0 &           4 &    lr &           3 & 0.696 &  0.663 &        0.848 &       
 0.478 & 0.700 & 0.797 \\
   0 &           5 &    lr &           4 & 0.679 &  0.642 &        0.848 &       
 0.435 & 0.683 & 0.826 \\
   1 &           1 &    lr &           5 & 0.804 &  0.787 &        0.825 &       
 0.750 & 0.892 & 0.845 \\
   1 &           2 &    lr &           6 & 0.839 &  0.831 &        0.850 &       
 0.812 & 0.919 & 0.872 \\
   1 &           3 &    lr &           7 & 0.804 &  0.787 &        0.825 &       
 0.750 & 0.892 & 0.856 \\
   1 &           4 &    lr &           8 & 0.786 &  0.738 &        0.850 &       
 0.625 & 0.850 & 0.822 \\
   1 &           5 &    lr &           9 & 0.804 &  0.769 &        0.850 &       
 0.688 & 0.872 & 0.833 \\
   2 &           1 &    lr &          10 & 0.732 &  0.713 &        0.818 &       
 0.609 & 0.750 & 0.742 \\
   2 &           2 &    lr &          11 & 0.768 &  0.750 &        0.848 &       
 0.652 & 0.778 & 0.764 \\
   2 &           3 &    lr &          12 & 0.768 &  0.757 &        0.818 &       
 0.696 & 0.794 & 0.823 \\
   2 &           4 &    lr &          13 & 0.732 &  0.720 &        0.788 &       
 0.652 & 0.765 & 0.773 \\
   2 &           5 &    lr &          14 & 0.696 &  0.683 &        0.758 &       
 0.609 & 0.735 & 0.758 \\
   3 &           1 &    lr &          15 & 0.714 &  0.695 &        0.771 &       
 0.619 & 0.771 & 0.774 \\
   3 &           2 &    lr &          16 & 0.750 &  0.733 &        0.800 &       
 0.667 & 0.800 & 0.788 \\
   3 &           3 &    lr &          17 & 0.643 &  0.619 &        0.714 &       
 0.524 & 0.714 & 0.686 \\
   3 &           4 &    lr &          18 & 0.750 &  0.733 &        0.800 &       
 0.667 & 0.800 & 0.777 \\
   3 &           5 &    lr &          19 & 0.625 &  0.605 &        0.686 &       
 0.524 & 0.706 & 0.680 \\
   4 &           1 &    lr &          20 & 0.800 &  0.723 &        0.972 &       
 0.474 & 0.778 & 0.801 \\
   4 &           2 &    lr &          21 & 0.727 &  0.680 &        0.833 &       
 0.526 & 0.769 & 0.781 \\
   4 &           3 &    lr &          22 & 0.709 &  0.629 &        0.889 &       
 0.368 & 0.727 & 0.773 \\
   4 &           4 &    lr &          23 & 0.691 &  0.627 &        0.833 &       
 0.421 & 0.732 & 0.754 \\
   4 &           5 &    lr &          24 & 0.727 &  0.680 &        0.833 &       
 0.526 & 0.769 & 0.785 \\
   0 &           1 &    rf &           0 & 0.732 &  0.694 &        0.909 &       
 0.478 & 0.714 & 0.792 \\
   0 &           2 &    rf &           1 & 0.714 &  0.685 &        0.848 &       
 0.522 & 0.718 & 0.801 \\
   0 &           3 &    rf &           2 & 0.732 &  0.694 &        0.909 &       
 0.478 & 0.714 & 0.782 \\
   0 &           4 &    rf &           3 & 0.732 &  0.700 &        0.879 &       
 0.522 & 0.725 & 0.739 \\
   0 &           5 &    rf &           4 & 0.714 &  0.692 &        0.818 &       
 0.565 & 0.730 & 0.760 \\
   1 &           1 &    rf &           5 & 0.875 &  0.856 &        0.900 &       
 0.812 & 0.923 & 0.848 \\
   1 &           2 &    rf &           6 & 0.821 &  0.800 &        0.850 &       
 0.750 & 0.895 & 0.842 \\
   1 &           3 &    rf &           7 & 0.839 &  0.812 &        0.875 &       
 0.750 & 0.897 & 0.850 \\
   1 &           4 &    rf &           8 & 0.839 &  0.812 &        0.875 &       
 0.750 & 0.897 & 0.809 \\
   1 &           5 &    rf &           9 & 0.804 &  0.750 &        0.875 &       
 0.625 & 0.854 & 0.806 \\
   2 &           1 &    rf &          10 & 0.804 &  0.787 &        0.879 &       
 0.696 & 0.806 & 0.825 \\
   2 &           2 &    rf &          11 & 0.857 &  0.839 &        0.939 &       
 0.739 & 0.838 & 0.839 \\
   2 &           3 &    rf &          12 & 0.786 &  0.772 &        0.848 &       
 0.696 & 0.800 & 0.816 \\
   2 &           4 &    rf &          13 & 0.804 &  0.781 &        0.909 &       
 0.652 & 0.789 & 0.847 \\
   2 &           5 &    rf &          14 & 0.804 &  0.787 &        0.879 &       
 0.696 & 0.806 & 0.831 \\
   3 &           1 &    rf &          15 & 0.750 &  0.714 &        0.857 &       
 0.571 & 0.769 & 0.826 \\
   3 &           2 &    rf &          16 & 0.696 &  0.700 &        0.686 &       
 0.714 & 0.800 & 0.783 \\
   3 &           3 &    rf &          17 & 0.768 &  0.738 &        0.857 &       
 0.619 & 0.789 & 0.848 \\
   3 &           4 &    rf &          18 & 0.786 &  0.762 &        0.857 &       
 0.667 & 0.811 & 0.812 \\
   3 &           5 &    rf &          19 & 0.768 &  0.729 &        0.886 &       
 0.571 & 0.775 & 0.793 \\
   4 &           1 &    rf &          20 & 0.782 &  0.734 &        0.889 &       
 0.579 & 0.800 & 0.836 \\
   4 &           2 &    rf &          21 & 0.764 &  0.695 &        0.917 &       
 0.474 & 0.767 & 0.798 \\
   4 &           3 &    rf &          22 & 0.727 &  0.655 &        0.889 &       
 0.421 & 0.744 & 0.848 \\
   4 &           4 &    rf &          23 & 0.764 &  0.708 &        0.889 &       
 0.526 & 0.780 & 0.798 \\
   4 &           5 &    rf &          24 & 0.800 &  0.760 &        0.889 &       
 0.632 & 0.821 & 0.777 \\
\end{tabular}
\end{adjustbox}
\caption{Evaluation metrics for Random Forest (rf) and Logisitc Regression (lr) 
during the nested cross validation}
\label{tab:storage}
\end{table}
