\section{Discussion of Results}
Although the classifiers perform worse than the ones in the original paper, 
most recorded metrics still certify them a good discriminative performance.
With a sensitivity of 90\% for both models and accuracy values between 73\% and 
76\%, the classifiers can still serve as supplementary decision tools to help 
physicians make decisions about allocating testing based on their predictions 
and other indicators. Further, the AUC and AUC (PRC) values which are over 80\% 
for both classifiers indicate that they are indeed strong classifiers.
Due to the slight imbalance of the data set, it is  favorable to examine the 
classifiers regarding this potential source of bias. If there was a significant 
difference between the accuracy and the balanced accuracy for a classifier, it 
would entail a bias against one of the target outcomes. This is not the case 
for both classifiers. Moreover, the high values of the AUC (PRC) suggest that 
the classifiers are not significantly affected by the imbalance of the data.
The metrics can also make statements about an infection event and the 
expressiveness of the classifiers. The high sensitivity signals that 90\% of 
patient who get a positive result actually have the disease. While the 
specificity is comparatively low, it does not necessarily pose a problem. A low 
specificity in combination with a high sensitivity indicates a higher false 
positive rate than a high false negative rate.\cite{RN168}
% rephrase this sentences, it is super weird
Therefore, if the result of the classifier is negative, it will be safe to 
assume that a PCR test for COVID-19 will also be negative and thus the patient 
will not have the disease.
This is more desirable because it is better to be over cautious and recommend 
more tests.
% maybe add PPV part from Schreibplan
\par
Since the question about the relevance of the classifier has been clarified, 
this paragraph deals with the internal classfification reasoning of the 
classifier and their accordance with findings from medical studies.
\begin{changemargin}{50pt}{50pt}
 Plausibility of models in terms of recorded metrics, i.e., sensitivity, 
specificity, ... and what that means for their predictive ``statements''
\\
Accordance of feature importance and scientific research (do most important 
features in model coincide with blood values identified as being evidence for a 
COVID-19 illness)
\end{changemargin}
\section{Discussion of Methods}
% auf non-normality eingehen und die zwei Methoden pr√§sentieren
\begin{changemargin}{50pt}{50pt}
Shortly mention that R implementation is not able to apply MICE model to other 
data only to data it is ``trained''/ ``fitted'' on (or maybe that's for the 
\ref{discussion} Discussion section)
\\
discuss imputation process further and its connection to cross-validation, 
keyword: data leakage
\\
discuss the original paper in light of replication and open science, e.g., lack 
of information about parameters/settings, missing information regarding 
results, ...
\end{changemargin}
\section{Conclusion}
