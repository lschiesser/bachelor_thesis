\section{Discussion of Results}
Although the classifiers perform worse than the ones in the original paper, 
most recorded metrics still certify them a good discriminative performance.
With a sensitivity of 90\% for both models and accuracy values between 73\% and 
76\%, the classifiers can still serve as supplementary decision tools to help 
physicians make decisions about allocating testing based on their predictions 
and other indicators. Further, the AUC and AUC (PRC) values which are over 80\% 
for both classifiers indicate that they are indeed strong classifiers.
Due to the slight imbalance of the data set, it is  favorable to examine the 
classifiers regarding this potential source of bias. If there was a significant 
difference between the accuracy and the balanced accuracy for a classifier, it 
would entail a bias against one of the target outcomes. This is not the case 
for both classifiers. Moreover, the high values of the AUC (PRC) suggest that 
the classifiers are not significantly affected by the imbalance of the data.
The metrics can also make statements about an infection event and the 
expressiveness of the classifiers. The high sensitivity signals that 90\% of 
patient who get a positive result actually have the disease. While the 
specificity is comparatively low, it does not necessarily pose a problem. A low 
specificity in combination with a high sensitivity indicates a higher false 
positive rate than a high false negative rate.\cite{RN168}
% rephrase this sentences, it is super weird
Therefore, if the result of the classifier is negative, it will be safe to 
assume that a PCR test for COVID-19 will also be negative and thus the patient 
will not have the disease.
This is more desirable because it is better to be over cautious and recommend 
more tests.
% maybe add PPV part from Schreibplan
\par
% this sentence sould be rewritten
Since the question about the relevance of the classifier has been clarified, 
this paragraph deals with the internal classification reasoning of the 
classifier and their accordance with findings from medical studies.
When inspecting the feature importance plots for the tree-based classifiers, 
both plots (Figure \ref{fig:rf_importance} and \ref{fig:dt_importance}) show 
that the AST, WBC, CRP and lymphocyte counts are under the five most important 
features.
% this sentence is also shit
In the Random Forest, the LDH count replaces the age variable in 
Decision Trees. For the Logistic Regression (Figure \ref{fig:lr_importance}), 
the eosinophil, WBC, basophil and lymphocyte counts are the most important 
features together with the age variable. All feature importance measures have 
the WBC and lymphocyte count in common. Multiple studies also show that these 
two blood values are significant indicators for a COVID-19 infection. The WBC 
or white blood cell count refers to the actual number of white blood cells per 
volume of blood while the lymphocytes are a subgroup of white blood cells 
involved in eliciting a immune response to foreign agents.\cite{RN137, RN188} 
During an infection with Sars-CoV-2, the WBC and lymphocyte counts will 
decrease since this is a sign of a viral infection and the body's response to 
it.\cite{RN162,RN186, RN185}
AST or aspartate aminotransferase, the most important feature in the tree-based 
methods, is an enzyme mainly found in heart and liver, its levels increase when 
the muscles of these organs are injured.\cite{RN189, RN188} This is also the 
case during a COVID-19 infection since the virus attacks not only the upper 
respiratory tracts but the whole body including the liver and 
heart.\cite{RN182} CRP or C-reactive protein is another substance found during 
a COVID-19 infection. It is produced in the liver and is discharged after 
tissue damage, the start of an infection or other inflammatory causes. 
Increased volumes of this protein are often the first indication of an 
infection or inflammation in the body.\cite{RN138, RN188} Elevated levels of 
this protein can also be observed during a COVID-19 infection since it is an 
infection and primarily targets lung tissue.\cite{RN187, RN162} LDH or lactate 
dehydrogenase is an enzyme involved in metabolic cycles for energy production, 
it is present in almost all cells in the body especially in the heart, 
liver, lungs, kidneys, muscles and blood cells. An increase in LDH can indicate 
acute kidney or liver disease, hypoxia, or heart and lung 
infarction.\cite{RN190, RN188} According to \cite{RN162, RN187}, a blood test 
reveals elevated levels of LDH because Sars-CoV-2 primarily attacks the upper 
respiratory tracts which leads to lung damage and the discharge of LDH into the 
blood stream. Age as the fourth important feature in the Decision Tree can also 
be a predictor for a positive result. \cite{RN193} determines that individuals 
older than 70 years are more susceptible to a severe course of the disease or 
even death. Further, the study revealed a higher risk for male individuals.
For eosinophils and basophils, \cite{RN162, RN185} reveal a significant 
decrease for patients with a positive test result. But \cite{RN162} notes that 
these differences might not have any clinical implication during diagnosis since 
the count in healthy individuals is also rather low and exhibits a large 
variability.
% rewrite this
In conclusion, the comparison of the most important features of every 
classifier with the findings from medical research reveals that the models use 
feature that medical research deemed as significant to identify patients with 
COVID-19. Thereby, they can be considered valid regarding their reasoning and 
classification in a theoretical or laboratory setting.
\par
% Real-world validation
The two paragraphs above certify the model statistical and theoretical 
validity, but they lack to prove that these models will also benefit medical 
personnel in the real world. Relational and ecological validity inspect how a 
model impacts the clinical workflow and the overall social environment. 
Specifically, relational validity refers to which extent different user groups, 
e.g., medical workers or patients, can relate to the model, i.e., how much they 
trust the predictions of the model \cite{RN151}. Users can either under-rely on 
the model ignoring all the predictions presented to them, or over-rely on the 
model accepting all predictions without questioning them. The former is 
disadvantageous because in this case the model would not have any benefit in a 
clinical setting. The latter could be seen as detrimental as this can lead to 
biased decision making or automation bias which could affect patients 
negatively \cite{RN151, RN152}.
Ecological validity investigates the impact of the technology further 
scrutinizing the intersection of the clinical and social settings a model is 
deployed in. A simple way to assess this type of validity is to compare 
performance-, outcome- and practice-oriented measures of different medical 
teams utilizing the model and relying on traditional technology in the same 
setting \cite{RN152}. Moreover, relational validity also assesses the 
sustainability of a model by continuously examining the effect of the model 
over time. As \citeauthor{RN198} \cite{RN198} notes ``unlike a drug or device, 
algorithms are not static products [as] their inputs […] can change with 
context''. Accordingly, AI models should be assessed periodically examining 
if they continue to provide comparable net benefits \cite{RN152}.
\section{Discussion of Methods}
% auf non-normality eingehen und die zwei Methoden präsentieren
The discussion of the methods is mainly focused on the imputation method MICE 
since it is a fairly new technique and is not commonly used to impute missing 
values.
\\
In order to asses the imputation procedure, this section will look at which 
method the imputation model used to impute the missing values and how the 
imputations compare to the original (non-imputed) data. The \code{mice()} 
package in R provides functions to inspect the imputation model used after the 
imputation is done. Since MICE can handle variables of different types and 
distributions and due to its individualistic approach, every variable can 
utilize a different model type to impute the missing values. In our case, the 
imputation method used the same model type for all variables as a consequence 
of the non-normality of the data called \textit{Predictive Mean Matching} (PMM).
PMM imputes missing values by using a small subset of complete observations 
called the donor pool. The donor pool contains 3 to 10 donor candidates which 
exhibit the same or similar predicted values compared to the predicted value of 
the missing entry. One donor is randomly selected from the candidates, and the
observed value of the donor is then used to replace the missing value 
\cite{RN144, RN145, RN146}. It is usually used to model data when the 
assumptions of normality and homoscedasticity are in questions, as is the case 
for the data used in this thesis \cite{RN146}.
\\
The imputations of the model can be assesed by generating summary statistics 
for the observed and the imputed values separately as well as jointly for each 
variable.
Especially comparing the differences in mean and standard deviation 
between the observed and imputed values of each variable can help identify 
variables of concern \cite{RN141}. When comparing the mean of the observed data 
with the mean of all samples of the imputed sets, most variables exhibit 
similar values for the summary statistics. Only ALP, GGT, LDH and Neutrophils 
exhibit apparent deviations from the observed data. It should be noted that 
most deviations are not significant.
When comparing the summary statistics of the observed data and only the imputed 
values for each data set, there are obvious deviations in every variable. 
However, it should be noted that with 70\% of samples missing at least one 
feature, it is hard to compare the statistics of the observed values with the 
imputed ones. Especially, since 117 of the 196 missing samples have a negative 
test result, which amounts to almost 60\% of the missing samples and 42\% of 
all samples, and only 79 of the missing samples have a negative test result. 
This imbalance is also reflected in the samples where all features are present. 
Of 83 complete samples only 8 samples have a negative test result and 60 have a 
positive result. Although the target variable was not available to the 
imputation model at the time of imputation, the characteristics of the target 
should have transpired in the data.
% this should definitley be reviewed
Since MICE operates on the characteristics 
of the data, this imbalance could be reflected in the imputation model. As 
reported in the section above the values for lymphocytes, eosinophils and 
basophils are decreased in patients who have the disease and therefore also 
have a positive test result. Inspecting the summary statistics of the 
imputed sets with this in mind, makes the lower mean and standard deviation 
more sensible.
It should also be noted that ALP and GGT have the highest missing value 
proportion. Since MICE has less data to base its imputations on, it can be 
expected taht the imputation exhibit apparent deviations from the observed 
values. MICE also accounts for the uncertainty during the impuatation process, 
meaning a higher number of missing values represents a higher uncertainty and 
therefore the imputation model is not able to make as educated guesses as for 
other variables with more observed data.
\\
In conclusion, it is very hard to make statements about the goodness of the 
imputation model. Most imputed sets do not exhibit drastic deviations from the 
observed set of observations and for those variables that show apparent 
deviations it can be ascribed to an imbalance when looking at the samples with 
missing values. Further, there is no standard procedure to asses the 
imputations produced by MICE and can therefore be very subjective. Intuitively, 
the imputations still seem like a fairly good approximation of the data.
For further research, I would nontheless recommend using another method to 
impute missing values that is more commonly used and has a theoretical basis. 
For example, \citeauthor{RN127} use K-nearest neighbor imputation in a 
subsequent study \cite{RN179} which uses a larger dataset than the one in 
\cite{RN127}.
\par
MICE as a method discussion
\begin{changemargin}{50pt}{50pt}
Shortly mention that R implementation is not able to apply MICE model to other 
data only to data it is ``trained''/ ``fitted'' on (or maybe that's for the 
\ref{discussion} Discussion section)
\\
discuss imputation process further and its connection to cross-validation, 
keyword: data leakage
\\
discuss the original paper in light of replication and open science, e.g., lack 
of information about parameters/settings, missing information regarding 
results, ...
\end{changemargin}
\section{Conclusion}
