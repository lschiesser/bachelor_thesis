\section{Discussion of Results}
Although the classifiers perform worse than the ones in the original paper, 
most recorded metrics still certify them a good discriminative performance.
With a sensitivity of 90\% for both models and accuracy values between 73\% and 
76\%, the classifiers can still serve as supplementary decision tools to help 
physicians make decisions about allocating testing based on their predictions 
and other indicators. Further, the AUC and AUC (PRC) values which are over 80\% 
for both classifiers indicate that they are indeed strong classifiers.
Due to the slight imbalance of the data set, it is  favorable to examine the 
classifiers regarding this potential source of bias. If there was a significant 
difference between the accuracy and the balanced accuracy for a classifier, it 
would entail a bias against one of the target outcomes. This is not the case 
for both classifiers. Moreover, the high values of the AUC (PRC) suggest that 
the classifiers are not significantly affected by the imbalance of the data.
The metrics can also make statements about an infection event and the 
expressiveness of the classifiers. The high sensitivity signals that 90\% of 
patient who get a positive result actually have the disease. While the 
specificity is comparatively low, it does not necessarily pose a problem. A low 
specificity in combination with a high sensitivity indicates a higher false 
positive rate than a high false negative rate.\cite{RN168}
% rephrase this sentences, it is super weird
Therefore, if the result of the classifier is negative, it will be safe to 
assume that a PCR test for COVID-19 will also be negative and thus the patient 
will not have the disease.
This is more desirable because it is better to be over cautious and recommend 
more tests.
% maybe add PPV part from Schreibplan
\par
% this sentence sould be rewritten
Since the question about the relevance of the classifier has been clarified, 
this paragraph deals with the internal classification reasoning of the 
classifier and their accordance with findings from medical studies.
When inspecting the feature importance plots for the tree-based classifiers, 
both plots (Figure \ref{fig:rf_importance} and \ref{fig:dt_importance}) show 
that the AST, WBC, CRP and lymphocyte counts are under the five most important 
features.
% this sentence is also shit
In the Random Forest, the LDH count replaces the age variable in 
Decision Trees. For the Logistic Regression (Figure \ref{fig:lr_importance}), 
the eosinophil, WBC, basophil and lymphocyte counts are the most important 
features together with the age variable. All feature importance measures have 
the WBC and lymphocyte count in common. Multiple studies also show that these 
two blood values are significant indicators for a COVID-19 infection. The WBC 
or white blood cell count refers to the actual number of white blood cells per 
volume of blood while the lymphocytes are a subgroup of white blood cells 
involved in eliciting a immune response to foreign agents.\cite{RN137, RN188} 
During an infection with Sars-CoV-2, the WBC and lymphocyte counts will 
decrease since this is a sign of a viral infection and the body's response to 
it.\cite{RN162,RN186, RN185}
AST or aspartate aminotransferase, the most important feature in the tree-based 
methods, is an enzyme mainly found in heart and liver, its levels increase when 
the muscles of these organs are injured.\cite{RN189, RN188} This is also the 
case during a COVID-19 infection since the virus attacks not only the upper 
respiratory tracts but the whole body including the liver and 
heart.\cite{RN182} CRP or C-reactive protein is another substance found during 
a COVID-19 infection. It is produced in the liver and is discharged after 
tissue damage, the start of an infection or other inflammatory causes. 
Increased volumes of this protein are often the first indication of an 
infection or inflammation in the body.\cite{RN138, RN188} Elevated levels of 
this protein can also be observed during a COVID-19 infection since it is an 
infection and primarily targets lung tissue.\cite{RN187, RN162} LDH or lactate 
dehydrogenase is an enzyme involved in metabolic cycles for energy production, 
it is present in almost all cells in the body especially in the heart, 
liver, lungs, kidneys, muscles and blood cells. An increase in LDH can indicate 
acute kidney or liver disease, hypoxia, or heart and lung 
infarction.\cite{RN190, RN188} According to \cite{RN162, RN187}, a blood test 
reveals elevated levels of LDH because Sars-CoV-2 primarily attacks the upper 
respiratory tracts which leads to lung damage and the discharge of LDH into the 
blood stream. Age as the fourth important feature in the Decision Tree can also 
be a predictor for a positive result. \cite{RN193} determines that individuals 
older than 70 years are more susceptible to a severe course of the disease or 
even death. Further, the study revealed a higher risk for male individuals.
For eosinophils and basophils, \cite{RN162, RN185} reveal a significant 
decrease for patients with a positive test result. But \cite{RN162} notes that 
these differences might not have any clinical implication during diagnosis since 
the count in healthy individuals is also rather low and exhibits a large 
variability.
% rewrite this
In conclusion, the comparison of the most important features of every 
classifier with the findings from medical research reveals that the models use 
feature that medical research deemed as significant to identify patients with 
COVID-19. Thereby, they can be considered valid regarding their reasoning and 
classification in a theoretical or laboratory setting.
\par
% Real-world validation
The two paragraphs above certify the model statistical and theoretical 
validity, but they lack to prove that these models will also benefit medical 
personnel in the real world. Relational and ecological validity inspect how a 
model impacts the clinical workflow and the overall social environment. 
Specifically, relational validity refers to which extent different user groups, 
e.g., medical workers or patients, can relate to the model, i.e., how much they 
trust the predictions of the model \cite{RN151}. Users can either under-rely on 
the model ignoring all the predictions presented to them, or over-rely on the 
model accepting all predictions without questioning them. The former is 
disadvantageous because in this case the model would not have any benefit in a 
clinical setting. The latter could be seen as detrimental as this can lead to 
biased decision making or automation bias which could affect patients 
negatively \cite{RN151, RN152}.
Ecological validity investigates the impact of the technology further 
scrutinizing the intersection of the clinical and social settings a model is 
deployed in. A simple way to assess this type of validity is to compare 
performance-, outcome- and practice-oriented measures of different medical 
teams utilizing the model and relying on traditional technology in the same 
setting \cite{RN152}. Moreover, relational validity also assesses the 
sustainability of a model by continuously examining the effect of the model 
over time. As \citeauthor{RN198} \cite{RN198} notes ``unlike a drug or device, 
algorithms are not static products [as] their inputs […] can change with 
context''. Accordingly, AI models should be assessed periodically examining 
if they continue to provide comparable net benefits \cite{RN152}.
\section{Discussion of Methods}
% auf non-normality eingehen und die zwei Methoden präsentieren
The discussion of the methods is mainly focused on the imputation method MICE 
since it is a fairly new technique and is not commonly used to impute missing 
values.
\\
In order to asses the imputation procedure, this section will look at which 
method the imputation model used to impute the missing values and how the 
imputations compare to the original (non-imputed) data. The \code{mice()} 
package in R provides functions to inspect the imputation model used after the 
imputation is done. Since MICE can handle variables of different types and 
distributions and due to its individualistic approach, every variable can 
utilize a different model type to impute the missing values. In our case, the 
imputation method used the same model type for all variables as a consequence 
of the non-normality of the data called \textit{Predictive Mean Matching} (PMM).
PMM imputes missing values by using a small subset of complete observations 
called the donor pool. The donor pool contains 3 to 10 donor candidates which 
exhibit the same or similar predicted values compared to the predicted value of 
the missing entry. One donor is randomly selected from the candidates, and the
observed value of the donor is then used to replace the missing value 
\cite{RN144, RN145, RN146}. It is usually used to model data when the 
assumptions of normality and homoscedasticity are in questions, as is the case 
for the data used in this thesis \cite{RN146}.
\\
The imputations of the model can be assessed by generating summary statistics 
for the observed and the imputed values separately as well as jointly for each 
variable.
Especially comparing the differences in mean and standard deviation 
between the observed and imputed values of each variable can help identify 
variables of concern \cite{RN141}. When comparing the mean of the observed data 
with the mean of all samples of the imputed sets, most variables exhibit 
similar values for the summary statistics. Only ALP, GGT, LDH and Neutrophils 
exhibit apparent deviations from the observed data. It should be noted that 
most deviations are not significant.
When comparing the summary statistics of the observed data and only the imputed 
values for each data set, there are obvious deviations in every variable. 
However, it should be noted that with 70\% of samples missing at least one 
feature, it is hard to compare the statistics of the observed values with the 
imputed ones. Especially, since 117 of the 196 missing samples have a negative 
test result, which amounts to almost 60\% of the missing samples and 42\% of 
all samples, and only 79 of the missing samples have a negative test result. 
This imbalance is also reflected in the samples where all features are present. 
Of 83 complete samples only 23 samples have a negative test result and 60 have 
a positive result. Although the target variable was not available to the 
imputation model at the time of imputation, the characteristics of the target 
should have transpired in the data.
% this should definitley be reviewed
Since MICE operates on the characteristics 
of the data, this imbalance could be reflected in the imputation model. As 
reported in the section above the values for lymphocytes, eosinophils and 
basophils are decreased in patients who have the disease and therefore also 
have a positive test result. Inspecting the summary statistics of the 
imputed sets with this in mind, makes the lower mean and standard deviation 
more sensible.
It should also be noted that ALP and GGT have the highest missing value 
proportion. Since MICE has less data to base its imputations on, it can be 
expected that the imputation exhibit apparent deviations from the observed 
values. MICE also accounts for the uncertainty during the imputation process, 
meaning a higher number of missing values represents a higher uncertainty and 
therefore the imputation model is not able to make as educated guesses as for 
other variables with more observed data.
\\
In conclusion, it is very hard to make statements about the goodness of the 
imputation model. Most imputed sets do not exhibit drastic deviations from the 
observed set of observations and for those variables that show apparent 
deviations it can be ascribed to an imbalance when looking at the samples with 
missing values. Further, there is no standard procedure to asses the 
imputations produced by MICE and can therefore be very subjective. Intuitively, 
the imputations still seem like a fairly good approximation of the data.
For further research, I would nonetheless recommend using another method to 
impute missing values that is more commonly used and has a theoretical basis. 
For example, \citeauthor{RN127} use K-nearest neighbor imputation in a 
subsequent study \cite{RN179} which uses a larger data set than the one in 
\cite{RN127}.
\\
The implementation of MICE used to impute the missing values is, as already 
mentioned, the R package \code{mice()} since the sklearn implementation was at 
the time not able to impute non-normal data correctly. The R implementation, 
although much better suited, has its own limitation. Namely, it is not able to 
apply the imputation model to other data only the data it is fitted on. 
Usually, when using any form of cross-validation the data is split into 
training and testing data before the imputation is done. The imputation method 
is then trained on the training data and the trained imputation model is 
applied to the training as well as the testing data. Using this process, the 
risk of data leakage is lowered.
% better description of data leakage
Data leakage is a term in machine learning 
that describes the contamination of a model with information from outside the 
training data where the model could learn something that it could not 
otherwise \cite{RN199}.
In the context of this thesis, data leakage could take place during the 
imputation process. Due to the limitation of the R implementation the whole 
data set is used to fit the imputation model because it is not possible to 
apply the model to different data sets. Therefore, characteristics from the 
test data are used to build the imputation for the training data and vice 
versa. However, the data is split into training and test data after the 
imputation, consequently lowering the risk of leakage. Moreover, it could be 
argued that it is not necessarily data leakage as no test data is used to train 
the classifier themselves but only to perform the imputation.
Even if it had been possible to fit the imputation model only on the training 
data and apply it to both the training and test data, the appropriateness would 
have been questionable. As MICE is built on the facets of the data, applying 
the imputation model based on the training data to test data could lead to 
false or unreliable imputations since the training data does not necessarily 
carry the characteristics of the test data. Hence, training the imputation 
model on the whole data set could be advantageous as it would encompass the 
characteristics of the whole data set. Additionally, training MICE on a very 
small sample size cannot account for the complex relationships between the 
features due to the reduced variability and sparseness of the data set.
Regardless of the implementation, MICE is also not a commonly used imputation 
method. Thus, there are no guidelines or procedure to identify a good 
imputations. Further, MICE lacks a theoretical basis since fitting a series of 
conditional probabilities may not be consistent with a proper joint 
distribution 
\cite{RN141}. The method relies on empirical studies rather than a theoretical 
basis to justify its procedure. In practice, however, this may not be a large 
issue, but further research is needed here. Despite these shortcomings, MICE 
provides a great advantage over other missing data techniques in terms of 
flexibility regarding variable type and number of variables to impute 
\cite{RN141, RN142}. After all, MICE should be used where traditional 
imputation methods face their limits due to small sample size and where the 
characteristics of the variables and their relations are still recognizable.
\par
% rewrite transition
Another explanation for the differing results of this re-implementation is the 
way the authors provided information on their implementation of the model 
training and selection process. Namely, they did not provide detailed 
information about which hyperparameters were tuned during cross-validation nor 
their ranges. Further, the structure of the cross-validation process was not 
clear at all times.
\\
Moreover, \citeauthor{RN127} do not report all evaluation metrics used to 
determine the best classifiers. They only report 95\% confidence intervals for 
the average accuracy and balanced accuracy as well as point references for the 
classifiers they deemed as best performing. There was no further information 
on other evaluation metrics like sensitivity or AUC. Additionally, the authors 
did only provide the configuration for one of the best performing models, 
namely the Random Forest. Unfortunately, this did not inform about the ranges 
for each hyperparameter used during Grid Search.
Nevertheless, the authors did provide their data and a description of each 
variable in the data set. It should also be noted that \cite{RN127} was only a 
feasibility study to explore if machine learning classifiers can help identify 
patients with COVID-19 solely based on a routine blood exam where the focus was 
not on replicability but feasibility. However, research like this that has a 
potential effect on an individual's health should strive to be as transparent 
and comprehensible as possible.
\section{Conclusion}
In conclusion, this thesis replicates two of the eight classifiers in 
\cite{RN127}. The classifiers predict the outcome of a PCR test for COVID-19 
using a routine blood exam. Although the classifiers perform worse than in the 
original paper, they still can be regarded as substantial due to a 90\% 
sensitivity and an accuracy above 70\% for both classifiers.
%rewrite this sentence (paper)
Further, a Decision Tree model was built to give a graphical representation of 
a possible classification reasoning.
%-----
A comparison with findings from medical research revealed that features 
identified as most important by the classifiers were also deemed pivotal in 
diagnosing COVID-19. Thereby, confirming the patterns found by the classifiers 
and certifying them a theoretical validity.
This implementation used Multiple Imputation using Chained Equations to impute 
the missing values. This method was used due to the small sample size of 279 
individuals and the proportionally high rate of sampels with at least one 
missing value. Though MICE lacks a theoretical basis and justifies its 
procedure with experimental studies, it provides great advantages especially 
where sample size is too small for more conventional and theoretically 
justified imputation methods and where the variables differ in terms of their 
type and distribution.
Seeing that the replicated paper was a feasibility study investigating if it is 
possible to predict a PCR test result from a range of blood values, further 
research should be done on this topic. For instance, \citeauthor{RN127} did a 
follow up study using a data set comprising more samples, more blood values and 
symptomatic observations at the time of triage or admittance to a hospital 
\cite{RN179}.
Importantly, it should always be brought to attention that these kind of models 
should never replace the actual medical test. They should be seen as a valuable 
though non-conclusive tool to detect patients with a certain disease earlier 
and better, resulting in improved management of these patients while awaiting 
confirmation by the medical test.
