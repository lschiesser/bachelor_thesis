\section{Data}
The data used to train the classifiers was provided by \citeauthor{RN127} \cite{RN127}. 
It was collected between the end of February 2020 and mid of March 2020 from patients admitted to the \textit{IRCSS Ospedale San Raffaele} and consists of 279 individuals who were selected randomly.
For each individual, the data set provides their age, gender, results of a 
routine blood screening, and the result of a PCR test for Sars-CoV-2.
A complete overview over the recorded features is provided in 
\ref{tab:overview-features}. The target variable \textit{Swab} is binary and 
indicates the result of a PCR-test for Sars-CoV-2 taken by naso-pharyngeal 
swab. A 0 indicates a negative test while a 1 indicates a positive test.
The data set is slightly imbalanced towards positive cases with 102 (37\%) 
negative cases and 177 (63\%) positive cases.
\\
Since the variable \textit{Gender} 
was provided as a string, it was transformed into two binary numerical 
variables called \textit{female} and \textit{male} by one-hot encoding.
Further, two values of the variable \textit{Age} were removed, specifically 
the values 0 and 1. This was sensible seeing that there was no other 
data recorded from minors under the age of 18 and thus these two values can 
be presumed to be input errors during the collection process.
% add recasting Lymphocytes because of input error
Further, the variable \textit{Lymphocytes} had to be recast using 
\code{pandas.to\_numeric} because there was an input error for one of its 
values rendering the contents of the column a string instead of a numerical 
value. This process created one missing values in place of the erroneous value.
\\
Table \ref{tab:feature-dist} provides common statistics for the numerical 
features of the data set.
% Rewrite this part, I think it's important but it does not sound good
All features are skewed. Blood values are 
skewed positively exhibiting a left-leaning distribution because for most blood 
values a lower or centered value is significant for a better health. Age 
is skewed negatively exhibiting a right-leaning distribution since COVID-19 
affected older individuals more severely than younger individuals especially in 
Italy. 
% non-normal distribution
As you can see in Figure \ref{fig:density}, most of the data is non-normally 
distributed.
By inspecting the kernel density plots provided in Figure \ref{fig:density}, it 
is apparent that most of the features are non-normally distributed. 
Additionally, a Shapiro-Wilk test for normality was performed to confirm this 
observation, its results are provided in table \ref{tab:shapiro-wilk}. 
% describe Shapiro-Wilk test shortly
% end with missing values since it's a good transition to MICE
Table \ref{tab:nan-overview} shows that most features have missing values. 196 
samples have at least one feature missing which amounts to 70 \% of the data. 
Due to the small size of data set it is not feasible to exclude these 
individuals from the analysis process. It is rather more constructive to use an 
imputation method that models the missing values based on the observed values in 
the data set. Therefore, \citeauthor{RN127} chose to use \textit{Multivariate 
Imputation by Chained Equations}. 
\section{Multivariate Imputation by Chained Equations}
\textit{Multivariate Imputation by Chained Equations} or \textit{MICE} for 
short is an imputation method proposed by \citeauthor{RN135} \cite{RN135}, it 
is also known as fully conditional specification (FCS).
MICE is a method that imputes missing data by estimating a set of possible 
values from distributions of observed data. Each variable with missing data 
$x_n$ is regressed on all other variables $x_1, ..., x_k$ which are restricted 
to the occurrences with observed data in $x_n$.
\\
The imputation process is based on the following four main steps \cite{RN142, 
RN141}: Firstly, all missing values are imputed using a simple imputation 
method (e.g. mean imputation). These imputations can be thought of as ``place 
holders'' used during the first modeling phase. During step 2, the ``place 
holder'' imputations for one variable \textit{x} are set back to missing. In 
step 3, all observed values from variable \textit{x} in step 2 are regressed on 
the other variables in the imputation model. Since this is the model building 
phase, this step only uses samples where \textit{x} has observed values. 
Therefore, \textit{x} is the dependent variable and all other variables are 
independent variables used in the regression model. In step 4, the missing 
values in \textit{x} are replaced with imputations (predictions) from the 
regression model built in step 3. All values of \textit{x}, the observed and the 
imputed values, are then used in subsequent regression models of other 
variables.
\\
Steps 2-4 are repeated for every variable with missing data. After the 
algorithm is done cycling through all variables, one iteration or ``cycle'' is 
completed. Steps 2-4 are repeated for a user-specified number of cycles. 
Generally, ten to twenty cycles should suffice to stabilize the results of the 
imputation that is the parameters controlling the imputations should have 
converged by then. This imputation process is usually repeated \textit{m} times 
creating \textit{m} slightly differently imputed data sets which are then used 
in the subsequent analysis. According to \cite{RN144, RN141, RN142}, already a 
small number of imputed data sets, usually three to ten, is sufficient to 
provide sensible results during analysis. It can, of course, be advantageous 
to use a higher number of imputed data sets to get a broader range of 
estimates, but setting \textit{m} higher requires more computations and storage 
and may not be worth preoccupying these extra resources \cite{RN144}.
\\
MICE assumes that the data is missing at random (MAR) that is the probability 
of data being missing does not depend on the unobserved data but is only 
dependent (conditional) on the observed data. Due to its individualistic 
approach, MICE can handle variables of different types using different 
modeling choices for different variable types. 
% auf non-normality eingehen und die zwei Methoden präsentieren
\begin{changemargin}{50pt}{50pt}
Describe MICE algorithm (keine Bewertung, findet in Discussion statt)
\\
Short description of implementation in Python using rpy2
\\
Shortly mention that R implementation is not able to apply MICE model to other 
data only to data it is ``trained''/ ``fitted'' on (or maybe that's for the 
\ref{discussion} Discussion section)
\\
Maybe include PMM?
\end{changemargin}
\section{Classifiers}
The original paper \cite{RN127} implements 7 classifiers.
\subsection{Decision Tree}
In order to explain Random Forests, we will take a closer look at Decision Trees since they are the basic building block of Random Forests and since we will use a Decision Tree during the validation to gain some insights into the workings of the Random Forest classifier.
A Decision Tree has a flowchart-like structure where each internal or decision node tests an attribute, each branch corresponds to one attribute value and each leaf node represents a classification. It is built up using the ID3 algorithm.
\\
ID3 tries to determine the best attribute of a given data set by the distribution of its values. The best attribute is then used as a root of the decision tree and a branch is created for every value this attribute can take which also creates a subset of the data set that only has the attribute value of the branch. This process is repeated for every branch with the remaining subset of the training set until a leaf node is reached.
\\
ID3 can use two different measures to decide which is the best attribute, information gain or the Gini index.
\\
Information Gain
\\
Gini index
\subsection{Random Forest}
The Random Forest Classifier is an ensemble classifier that uses multiple Decision Tree instances to classify the given data. It uses two methods to diversify the different classification of the DT instances called tree bagging and feature subset selection.
\subsection{Logistic regression}
\section{Model training}
The two models are trained using 5-fold nested cross validation.
In each outer fold of the nested cross validation, the data is imputed using 
MICE and then split into training and test data. Since MICE usually generates 
several imputed data sets to account for the uncertainty in the imputation 
process, there are 5 different models for each classifier per fold.
\\
The models are then evaluated using accuracy, balanced accuracy, sensitivity, 
specificity, positive predictive value, and area under the ROC curve. Balanced 
accuracy is used in addition to the average accuracy due to the slight 
imbalance in the data set. If the balanced accuracy differs significantly from 
the average accuracy, the imbalance has an effect on the classifier regarding 
class prevalence. Assuming the standard layout of a confusion matrix in a 
binary case, the balanced accuracy is defined as the average of sensitivity and 
specificity 
\begin{equation}
 balanced \ accuracy = \frac{\frac{TP}{TP+FN}\frac{TN}{TN+FP}}{2}
 \label{eq:balanced_acc}
\end{equation}
Sensitivity is interpreted as the proportion of people infected with a disease 
who will test positive for this disease. It is calculated as shown in Equation 
\ref{eq:sensitivity}. Since it is only calculated using the part of the 
population with the disease, sensitivity can only give evidence about the true 
positive rate and not the false positive rate of a test. \cite{RN168}
\begin{equation}
 sensitivity = \frac{TP}{TP+FN}
 \label{eq:sensitivity}
\end{equation}
Specificity is defined as the proportion of people without the disease and are 
identified correctly, i.e., who tested negative for a disease. It is calculated 
as show in Equation \ref{eq:specificity}. Specificity only gives information 
about the proportion of people without the disease and a negative test and 
therefore cannot disclose anything about the false negative rate. \cite{RN168}
\begin{equation}
 specificity = \frac{TN}{TN+FP}
 \label{eq:specificity}
\end{equation}
The positive predictive value (PPV) measures the post-test probability of the 
disease given a positive test. It is calculated as shown in 
Equation \ref{eq:ppv}. \cite{RN168}
\begin{equation}
 PPV = \frac{TP}{TP+FP}
 \label{eq:ppv}
\end{equation}
\\
The hyperparameters of the best performing models for Random Forest and 
Logistic Regression respectively are then used to obtain the final classifiers.
For the validation, the whole data set is split into training and validation 
data according to a 80:20 split. The data set is imputed before the split 
due to the working of the \code{mice()} package in R. The classifiers are 
trained on the data set and then validated using the validation data. 
Additionally to the evaluation metrics above, the final classifiers were also 
evaluated using the ROC curve and the precision-recall curve. Additionally, the 
precision-recall curve is used to assess the performance of the classifiers 
regarding the slight imbalance in the data set.
\begin{changemargin}{50pt}{50pt}
 Describe here k-fold nested cross validation, evaluation metrics used
 \\
 Here maybe also implementation of MICE in Python using rpy2
\end{changemargin}
