\section{Data}
The data used to train the classifiers was provided by 
\citeauthor{RN127} \cite{RN127}. 
It was collected between the end of February 2020 and mid of March 2020 from 
patients admitted to the \textit{IRCSS Ospedale San Raffaele} in Milan, Italy 
and consists of 279 individuals who were selected randomly.
For each individual, the data set provides their age, gender, the results of a 
routine blood screening, and a PCR test for Sars-CoV-2.
A complete overview of the recorded variables is provided in Table 
\ref{tab:overview-features}. The target variable \textit{Swab} is binary and 
indicates the result of a PCR-test for Sars-CoV-2 taken by a nasopharyngeal 
swab. 0 indicates a negative test, while 1 indicates a positive test.
The data set is slightly imbalanced towards positive cases, with 102 (37\%) 
negative cases and 177 (63\%) positive cases.
\\
The data was cleaned and wrangled in order to properly analyze it.
Since the variable \textit{Gender} 
was provided as a string, it was transformed into two binary numerical 
variables called \textit{female} and \textit{male} by one-hot encoding.
Further, two values of the variable \textit{Age} were replaced with a missing 
value indicator, specifically 
the values 0 and 1. This was sensible, seeing that there was no other 
data recorded from minors. Thus these two values can 
be presumed to be input errors during the collection process.
% add recasting Lymphocytes because of input error
Further, the variable \textit{Lymphocytes} had to be recasted to a numeric 
format because there was an input error for one of its 
values rendering the column's contents a string instead of a numerical 
value. This process created one missing value in place of the erroneous value.
\\
Table \ref{tab:feature-dist} provides standard statistics for the numerical 
features of the data set, specifically their measurement unit, mean, standard 
deviation and median.
% Rewrite this part, I think it's important but it does not sound good
These standard statistics in combination with the kernel density plots 
provided in Figure \ref{fig:density} show that all features are skewed 
relative to a (standard) normal distribution. Blood values are 
skewed positively, exhibiting a right-tailed distribution because, for most 
blood values, a lower or centered value is a sign of better health. Age 
is skewed negatively, exhibiting a left-tailed distribution since COVID-19 
affected older individuals more severely than younger individuals, especially 
in Italy \cite{RN193}. 
\\
In order to confirm the observation of non-normality, a Shapiro-Wilk test for 
normality was performed.
The test probes the null-hypothesis that the samples of a variable come from a 
normally distributed population. If the test's p-value is smaller than 
0.05, a commonly chosen $\alpha$ level, the null-hypothesis is rejected, 
meaning the data is not normally distributed \cite{RN196}.
The test results are provided in Table \ref{tab:shapiro-wilk} and show 
that all of the variables are non-normally distributed.
\\
% end with missing values since it's a good transition to MICE
Table \ref{tab:nan-overview} shows that most features have missing values. 196 
samples have at least one feature missing, which amounts to 70\% of the data. 
Due to the small data set size, it is not feasible to exclude these 
individuals from the analysis process. It is rather more constructive to use an 
imputation method that models the missing values based on the data set's 
observed values. Therefore, \citeauthor{RN127} chose to use 
\textit{Multivariate 
Imputation by Chained Equations}. 
\section{Multivariate Imputation by Chained Equations}
\textit{Multivariate Imputation by Chained Equations} or \textit{MICE} for 
short is an imputation method proposed by \citeauthor{RN135} \cite{RN135}; it 
is also known as \textit{fully conditional specification} (FCS).
MICE is a method that imputes missing data by estimating a set of possible 
values from distributions of observed data. Each variable with missing data 
$x_n$ is regressed on all other variables $x_1, ..., x_n$ which are restricted 
to the occurrences with observed data in $x_k$.
MICE assumes that the data are missing at random (MAR). That is, the 
probability 
of missing data does not depend on the unobserved data but is only 
dependent (conditional) on the observed data. Due to its individualistic 
approach, MICE can handle variables of different types using different 
modeling choices for different variable types.
\\
The imputation process is based on the following four main steps \cite{RN142, 
RN141}: Firstly, all missing values are imputed using a simple imputation 
method (e.g., mean imputation). These imputations can be thought of as ``place 
holders'' used during the first modeling phase. During step 2, the ``place 
holder'' imputations for one variable \textit{x} are set back to missing. In 
step 3, all observed values from variable \textit{x} in step 2 are regressed on 
the imputations model's other variables. Since this is the model building 
phase, this step only uses samples where \textit{x} has observed values. 
Therefore, \textit{x} is the dependent variable, and all other variables are 
independent variables used in the regression model. In step 4, the missing 
values in \textit{x} are replaced with imputations (predictions) from the 
regression model built in step 3. All values of \textit{x}, the observed and the 
imputed values, are then used in subsequent regression models of other 
variables.
\\
Steps 2-4 are repeated for every variable with missing data. After the 
algorithm cycled through all variables, one iteration is 
completed. Steps 2-4 are repeated for a user-specified number of cycles. 
Generally, ten to twenty cycles should suffice to stabilize the imputation 
results, indicating that the parameters controlling the imputations should have 
converged by then. This imputation process is usually repeated \textit{m} 
times, creating \textit{m} slightly differently imputed data sets, which are 
then used in the subsequent analysis. According to \cite{RN144, RN141, RN142}, 
already a 
small number of imputed data sets, usually three to ten, is sufficient to 
provide sensible results during analysis. It can, of course, be advantageous 
to use a higher number of imputed data sets to get a broader range of 
estimates. However, setting \textit{m} higher requires more computations and 
storage 
and may not be worth preoccupying these extra resources \cite{RN144}.

\section{Classifiers}
The original paper implements eight classifiers, namely Random 
Forest, Logistic Regression, Decision Tree, K-nearest Neighbors, Naive Bayes, 
Support Vector Machines, Extremely Randomized Trees, and three-way Random 
Forest \cite{RN127}. 
Here, I will reproduce the Random Forest, Logistic Regression, and Decision 
Tree classifiers. The following section introduces them.
\subsection{Decision Tree}
Since Decision Trees are the basic building block of Random Forest and we will 
use them during the validation it is necessary to take a closer look at them 
\cite{RN163}. 
Thereby the use of Random Forests will be motivated and we will gain some 
insights into the workings of the Random Forest Classifier.
A Decision Tree has a flowchart-like structure where each internal or decision 
node tests an attribute. Each branch corresponds to one attribute value, and 
each leaf node represents a classification. It is built up using the ID3 
algorithm \cite{RN171}.
\\
ID3 tries to determine the best attribute of a given data set by the 
distribution of its values. The best attribute is then used as a root of the 
Decision Tree, and a branch is created for every value this attribute can take, 
which also creates a subset of the data set that only has the attribute value 
of the branch. This process is repeated for every branch with the remaining 
subset of the training set until a leaf node is reached. \cite{RN165}
\\
ID3 can use many different measures to decide which is the best attribute. Two 
of the most popular ones are Information Gain and the Gini Impurity.
\\
Information Gain uses the entropy measure to compute the impurity also known as 
heterogeneity of an attribute. Entropy originates from information theory and 
describes the average 
information content of an attribute's possible outcome, it is calculated, as 
shown in Equation \ref{eq:entropy} where S is a set of samples and C are the 
possible outcome labels of the target variable. A high entropy represents a 
high average 
information content in an attribute \cite{RN167}. Information Gain measures the 
expected 
reduction in entropy of a set S caused by learning the state of a random 
attribute A.  It is calculated, as shown in Equation \ref{eq:gain}, and can be 
described as the difference between the entropy of a set S and the 
weighted average of the child entropies $E(S_v)$ where $S_v$ is a subset of S 
where the attribute A has the value v. This is computed for all remaining 
attributes, the attribute that maximizes the differences is then selected as a 
new node \cite{RN165}.
\begin{equation}
 E(S) = \sum_{i=1}^C -p(i) \ log_2(p(i))
 \label{eq:entropy}
\end{equation}
\begin{equation}
 Gain(S, A) = E(S) - \sum_{v}^{V} \frac{|S_v|}{S} E(S_v)
 \label{eq:gain}
\end{equation}
\\
The Gini Impurity or Gini index measures the impurity in a set S that is the 
probability of incorrect classification of a random sample. It is calculated 
as shown in Equation \ref{eq:gini_impurity}. The measure on which the algorithm 
bases its decision which attribute to select as a node is called the Gini 
gain. It is calculated as shown in Equation \ref{eq:gini_gain} and can be 
described as the average Gini Impurity. The attribute with the lowest value is 
selected for the Gini gain since we want to minimize the attributes' incorrect 
classification \cite{RN171}.
\begin{equation}
 G = 1 - \sum_{i=1}^C p(i)^2
 \label{eq:gini_impurity}
\end{equation}
\begin{equation}
 Gini(S, A) = \sum_{i=1}^C \frac{|S_i|}{|S|} G(S_i)
 \label{eq:gini_gain}
\end{equation}
\\
% maybe throw this out?
%ID3 is not able to process numerical attributes and missing values. It must 
%therefore be extended to be able to deal with real-world data by the C4.5 
%algorithm.
\subsection{Random Forest}
The Random Forest Classifier is an ensemble classifier that uses multiple 
Decision Tree instances to classify the given data. It uses two methods to 
diversify the different classifications of the Decision Tree instances called 
tree bagging and feature bagging \cite{RN163}.
\\
During tree bagging, the Decision Tree is not trained on the whole training 
set but a subset of the training set. The subsets are generated by sampling 
the original data set with replacement. A certain percentage of samples 
is selected from the original data set and the remaining 
percentage are duplicates of the already selected samples. This 
process generates new bagged training sets $S_i$ with the same size as the 
original training set $S$. If the size of the bagged set is smaller than the 
size of the original set, this process is called sub-bagging \cite{RN163, 
RN170}.
\\
Feature bagging or feature subset selection limits the number of features the 
individual Decision Trees can use at each new split in consideration. 
For each decision node consideration, several features are randomly 
selected and presented to the Decision Tree algorithm. A hyperparameter 
controls the number of features available to the algorithm. For 
classification problems, this is usually set to $\sqrt{p}$ where p is the 
number of features present in the complete data set \cite{RN163, RN166}.
\\
Each Decision Tree in the forest generates a classification. A majority vote 
over all Decision Trees determines the final classification of the Random 
Forest. Using tree and feature bagging, Random Forests average over 
all individual Decision Tree models, reducing the variance of 
classification and avoiding overfitting \cite{RN166}.
\subsection{Logistic Regression}
Logistic Regression is a modification of linear regression that models the 
probability $p(X)$ that a sample $X$ has the label 1. To do so, it uses a 
special form of the sigmoid function called the logistic function where 
$\mathbf{X}$ is a vector or matrix of samples and $\vec{\beta}$ is a vector of 
the coefficients \cite{RN170, RN166}:
\begin{equation}
 p(\mathbf{X}) = \phi_{sig}(\mathbf{X}) = 
\frac{e^{\mathbf{X}\vec{\beta}}}{1+e^{\mathbf{X}\vec{\beta}}} = 
\frac{1}{1+e^{-\mathbf{X}\vec{\beta}}}
 \label{eq:lf}
\end{equation}
After some manipulation, we can see
\begin{equation}
 \frac{p(\mathbf{X})}{1-p(\mathbf{X})} = e^{\mathbf{X}\vec{\beta}}
 \label{eq:odds}
\end{equation}
The left-hand side of Equation \ref{eq:odds} is called odds. It can take a 
value between 0 and $\infty$ indicating very low or very high probabilities 
of having the label 1.
After applying the logarithm to both sides of Equation \ref{eq:odds}, the 
log-odds or logit is obtained (Equation \ref{eq:logit}), which is linear in X 
\cite{RN174, RN166}.
\begin{equation}
 ln \Big( \frac{p(\mathbf{X})}{1-p(\mathbf{X})} \Big) = \mathbf{X}\vec{\beta}
 \label{eq:logit}
\end{equation}
Since $\beta_0$ and all $\beta_i$ of the vector $\vec{\beta}$ are unknown, 
they must be estimated on the 
available training data. The method that is used to achieve this is called 
maximum likelihood estimation (MLE). MLE tries to find estimates for the 
coefficients $\beta$ such that the estimated probability $\hat{p}(x_i)$ of 
having label 1 for each individual is as close as possible to the individual's 
observed label. In other words, MLE tries to find estimates for the 
coefficients $\beta$ of the model $P(\mathbf{X})$ described in Equation 
\ref{eq:lf} such that the resulting probability is close to one if the 
individual's observed label is 1 and close to zero if the individual's label is 
0. This can be formalized as a likelihood function as shown in Equation 
\ref{eq:likelihood}. The estimates for $\beta$ are then chosen to maximize the 
likelihood function \cite{RN166}.
\begin{equation}
 L(\vec{\beta}|\vec{y}, \mathbf{X}) = \prod_{i} p(x_i)^{y_i} 
(1-p(x_i))^{(1-y_i)}
\label{eq:likelihood}
\end{equation}
\section{Model Training}
Random Forest and Decision Tree are trained using 5-fold nested cross 
validation.
In each outer fold of the nested cross validation, the data is imputed using 
MICE and then split into training and test data. Since MICE usually generates 
several imputed data sets to account for the uncertainty in the imputation 
process, there are 5 different models for each classifier per fold. The cross 
validation's inner fold is used to determine the best hyperparameter 
combination for the classifiers using \textit{Grid Search}, also called 
hyperparameter tuning \cite{RN191}. Figure \ref{fig:cv} explains this process 
in further detail. The parameters for Random Forest are:
% maybe add justification of why to use these parameters
\begin{itemize}
 \item Number of trees in the forest: [10, 100, 500]
 \item Number of features to consider looking for the best split: [4, 8, 12, 16]
 \item Quality of split measure: Gini Impurity or Information Gain
\end{itemize}
For Logistic Regression, Grid Search seeks for an optimum among the 
following hyperparameters:
\begin{itemize}
 \item Inverse of regularization strength C: [0.001, 0.01, 0.1, 1, 10, 100, 
1000]
 \item Penalty: L1 or L2 regularization
 \item Algorithm to use during optimization problem: liblinear or saga
 \item Maximal iteration: [200, 300, 400, 500]
\end{itemize}
The remaining hyperparameters use the default values specified in the 
documentation of sklearn for Random Forests and Logistic Regression 
\cite{RN191}.
Grid Search selects the best hyperparameters according to the highest accuracy 
value.
\\
After the training, the models are evaluated using accuracy, balanced 
accuracy, sensitivity, 
specificity, positive predictive value, and area under the ROC curve which are 
explained in the next section.
The hyperparameters of the best performing models for Random Forest and 
Logistic Regression are then used to obtain the final classifiers.
For the validation, the whole data set is split into training and validation 
data according to an 80:20 ratio. The data set is imputed 
before the split 
due to how the \code{mice()} package runs in R. The classifiers are 
trained on the training data and then validated using the validation data. 
In addition to the evaluation metrics above, the final classifiers were also 
evaluated using the ROC curve and the precision-recall curve. Additionally, the 
precision-recall curve is used to assess the classifiers' performance 
regarding the slight imbalance in the data set. To further gain some insights 
into the Random Forest's classification reasoning, a Decision 
Tree is trained using the standard hyperparameters according to the sklearn 
implementation \cite{RN191}.
\\
To implement this procedure, the Python libraries scikit-learn, pandas, and 
numpy are used. To implement the imputation method, the R library \code{mice()} 
is used and connected to the Python implementation using a bridge package 
called rpy2. The R implementation of the imputation method is used instead of a 
Python implementation since it provides more functions to inspect the MICE 
procedure and since it is able to handle non-normally distributed data. To 
visualize the results, the libraries seaborn and matplotlib are used.

\section{Evaluation Metrics}
The following terms will be used in the equations to describe the evaluation 
metrics: True Positive, False Positive, False Negative, and 
True Negative. They are further characterized in Table 
\ref{tab:confusion-matrix}. 
\begin{table}[h]
\centering
\begin{tabular}{ll|c|c|}
\cline{3-4}
 &  & \multicolumn{2}{c|}{True condition}                                        
       \\ \cline{3-4} 
 &  & \multicolumn{1}{l|}{Condition positive} & \multicolumn{1}{l|}{Condition 
negative} \\ \hline
\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Predicted\\ 
condition\end{tabular}}} & \begin{tabular}[c]{@{}l@{}}Condition\\ predicted\\ 
positive\end{tabular} & \begin{tabular}[c]{@{}c@{}}True Positive\\ 
(TP)\end{tabular} & \begin{tabular}[c]{@{}c@{}}False Positive\\ 
(FP)\end{tabular} \\ \cline{2-4} 
\multicolumn{1}{|l|}{}                                     & 
\begin{tabular}[c]{@{}l@{}}Condition\\ predicted\\ negative\end{tabular} & 
\begin{tabular}[c]{@{}c@{}}False Negative\\ (FN)\end{tabular} & 
\begin{tabular}[c]{@{}c@{}}True Negative\\ (TN)\end{tabular}  \\ \hline
\end{tabular}
\caption{Confusion matrix for binary classification}
\label{tab:confusion-matrix}
\end{table}
\\
%add accuracy here
Accuracy describes the closeness of a predicted label to its true 
label. It is calculated as shown in Equation \ref{eq:accuracy}. Accuracy alone 
is not a sensible measure to determine the goodness of fit for a model since 
the model can be affected by imbalanced data or poor parameter initialization. 
Accordingly, other measures should be considered during model 
selection \cite{RN167}.
\begin{equation}
 accuracy = \frac{TP + TN}{TP+FP+FN+TN}
 \label{eq:accuracy}
\end{equation}
Balanced accuracy is used in addition to the average accuracy due to the slight 
imbalance in the data set. If the balanced accuracy differs significantly from 
the average accuracy, the imbalance affects the classifier regarding 
class prevalence. The balanced accuracy is defined as the average of 
sensitivity and specificity \cite{RN167, RN127}.
\begin{equation}
 balanced \ accuracy = \frac{\frac{TP}{TP+FN}\frac{TN}{TN+FP}}{2}
 \label{eq:balanced_acc}
\end{equation}
Sensitivity is interpreted as the proportion of people infected with a disease 
who will test positive for this disease. It is calculated as shown in Equation 
\ref{eq:sensitivity}. Since it is only calculated using the part of the 
population with the disease, sensitivity can only give evidence about the true 
positive rate and not the test's false positive rate \cite{RN168}.
\begin{equation}
 sensitivity = \frac{TP}{TP+FN}
 \label{eq:sensitivity}
\end{equation}
Specificity is defined as the proportion of people without the disease that are 
identified correctly, i.e., who tested negative for the disease. It is 
calculated as shown in Equation \ref{eq:specificity}. Specificity only gives 
information about the proportion of people without the disease and a negative 
test and cannot disclose anything about the false negative 
rate \cite{RN168}.
\begin{equation}
 specificity = \frac{TN}{TN+FP}
 \label{eq:specificity}
\end{equation}
The positive predictive value (PPV) measures the post-test probability of the 
disease given a positive test. It, therefore, refers to the probability that a 
subject with a positive test has indeed the disease. It is calculated as shown 
in Equation \ref{eq:ppv} \cite{RN168}.
\begin{equation}
 PPV = \frac{TP}{TP+FP}
 \label{eq:ppv}
\end{equation}
The receiver operating characteristic curve (ROC curve) is a commonly used 
metric to determine a model's performance describing the trade-off between 
specificity and sensitivity. It is calculated using the prediction scores of a 
model that are either discriminant values or posterior 
probabilities \cite{RN161}. Most models do not produce a classification label 
as their output but rather a prediction score that is then thresholded to 
correspond to one of the target variable levels. The metrics mentioned above 
are all 
so called single-thresholded metrics since they are defined only for the 
classifier's thresholded output \cite{RN161, RN167}. Since ROC uses the 
prediction scores instead of the thresholded output of a model, it can compute 
the sensitivity and specificity for every sensible threshold. The resulting 
values are then plotted with sensitivity or true positive rate as the vertical 
axis and the false positive rate or 1 - specificity as the 
horizontal axis. The points are interpolated linearly to create the ROC curve  
\cite{RN161}. Usually, a reference line is added to the plot illustrating a 
model's performance that only makes random predictions. The curve for the 
trained model is expected to be above the reference line at all times. 
% add strenght of predicitive model ...
A model with perfect performance will appear in the top left-hand corner 
of the ROC space where the sensitivity is one, and the false positive rate is 
zero \cite{RN167, RN159}.
\\
The area under the ROC curve can also be measured and is called either AUC for 
area under the ROC curve or ROC index. It can give a numeric summary of the ROC 
curve. It is calculated using the ROC curve's integral which can be easily 
done using the trapezoidal method since the ROC curve is discrete and stepped. 
A large AUC value indicates a better model performance. As a rule of thumb, an 
AUC over 0.7 indicates a strong predictive model, and an AUC below 0.6 
indicates 
a weak model. AUC and ROC both are quite robust against imbalanced data, but 
other non-single-threshold metrics can be used in this case as 
well \cite{RN167, RN159}.
\par
The Precision-Recall curve (PRC) is another non-single-threshold metric that is 
especially useful when dealing with an imbalanced data set. It calculates the 
precision and recall values for each feasible threshold. The 
resulting values are then plotted with precision, also called positive 
predictive value, as the vertical axis and recall, also called sensitivity, as 
the horizontal axis. The points are interpolated non-linearly to create a 
curve. 
In contrast to the ROC curve, which uses a fixed reference line, the PRC's 
baseline is determined by the ratio of positive (P) and negative (N) samples in 
the data set and represented as $y = \frac{P}{P + N}$. Each point in the PRC 
plot corresponds to exactly one point in the ROC plot since one of the measures 
in the plot is the same. The area under the curve can also be 
calculated for the precision-recall curve similar to the AUC of the ROC curve 
and is then denoted by AUC (PRC) or average precision. Similarly, a large value 
for the AUC (PRC) indicates a strong model, and a lower value indicates a weak 
model \cite{RN160, RN161}.
