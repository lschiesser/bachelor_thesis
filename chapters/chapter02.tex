\section{Data}
The data used to train the classifiers was provided by \citeauthor{RN127} \cite{RN127}. 
It was collected between the end of February 2020 and mid of March 2020 from patients admitted to the \textit{IRCSS Ospedale San Raffaele} and consists of 279 individuals who were selected randomly.
For each individual, the data set provides their age, gender, results of a 
routine blood screening, and the result of a PCR test for Sars-CoV-2.
A complete overview over the recorded features is provided in 
\ref{tab:overview-features}. The target variable \textit{Swab} is binary and 
indicates the result of a PCR-test for Sars-CoV-2 taken by naso-pharyngeal 
swab. A 0 indicates a negative test while a 1 indicates a positive test.
The data set is slightly imbalanced towards positive cases with 102 (37\%) 
negative cases and 177 (63\%) positive cases.
\\
Since the variable \textit{Gender} 
was provided as a string, it was transformed into two binary numerical 
variables called \textit{female} and \textit{male} by one-hot encoding.
Further, two values of the variable \textit{Age} were removed, specifically 
the values 0 and 1. This was sensible seeing that there was no other 
data recorded from minors under the age of 18 and thus these two values can 
be presumed to be input errors during the collection process.
\\
Table \ref{tab:feature-dist} provides common statistics for the numerical 
features of the data set.
% non-normal distribution
As you can see in Figure \ref{fig:density}, most of the data is non-normally 
distributed.
% end with missing values since it's a good transition to MICE
Table \ref{tab:nan-overview} shows that most features have missing values. 196 
samples have at least one feature missing which amounts to 70 \% of the data. 
Due to the small size of data set it is not feasible to exclude these 
individuals from the analysis process. It is rather more constructive to use an 
imputation method that models the missing values based on the observed values in 
the data set. Therefore, \citeauthor{RN127} chose to use \textit{Multivariate 
Imputation by Chained Equations}. 
\section{Multivariate Imputation by Chained Equations}
\textit{Multivariate Imputation by Chained Equations} or \textit{MICE} for 
short is an imputation method proposed by \citeauthor{RN135} \cite{RN135}, it 
is also known as fully conditional specification (FCS).
MICE is a method that imputes missing data by estimating a set of possible 
values from distributions of observed data. Each variable with missing data 
$x_n$ is regressed on all other variables $x_1, ..., x_k$ which are restricted 
to the occurrences with observed data in $x_n$.
\\
The imputation process is based on the following four main steps \cite{RN142, 
RN141}: Firstly, all missing values are imputed using a simple imputation 
method (e.g. mean imputation). These imputations can be thought of as ``place 
holders'' used during the first modeling phase. During step 2, the ``place 
holder'' imputations for one variable \textit{x} are set back to missing. In 
step 3, all observed values from variable \textit{x} in step 2 are regressed on 
ther other variables in the imputation model. Since this is the model building 
phase, this step only uses samples where \textit{x} has observed values. 
Therefore, \textit{x} is the dependent variable and all other variables are 
independent variables used in the regression model. In step 4, the missing 
values in \textit{x} are replaced with imputations (predictions) from the 
regression model built in step 3. All values of \textit{x}, the observed and the 
imputed values, are then used in subsequent regression models of other 
variables.
Steps 2-4 are repeated for every variable with missing data. After the 
algorithm is done cycling through all variables, one iteration or ``cycle'' is 
completed. Steps 2-4 are repeated for a user-specified number of cycles. 
Generally, ten to twenty cycles should suffice to stabilize the results of the 
imputation that is the parameters controlling the imputations should have 
converged by then.
\begin{changemargin}{50pt}{50pt}
Describe MICE algorithm (keine Bewertung, findet in Discussion statt)
\\
Short description of implementation in Python using rpy2
\\
Shortly mention that R implementation is not able to apply MICE model to other 
data only to data it is ``trained''/ ``fitted'' on (or maybe that's for the 
\ref{discussion} Discussion section)
\\
Maybe include PMM?
\end{changemargin}
\section{Model selection}
\subsection{Random Forest}
\subsection{Logistic regression}
\section{Model training}
\begin{changemargin}{50pt}{50pt}
 Describe here k-fold nested cross validation, evaluation metrics used
 \\
 Here maybe also implementation of MICE in Python using rpy2
\end{changemargin}
